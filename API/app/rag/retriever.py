# app/rag/retriever.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
import os
import logging
import boto3

from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth
from app.services.common import Mode
from app.config import settings
import json
from typing import List, Dict, Any
from starlette.concurrency import run_in_threadpool

# watsonx.ai (pip install ibm-watsonx-ai)
try:
    from ibm_watsonx_ai import Credentials, APIClient
    from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
    from ibm_watsonx_ai.foundation_models import ModelInference
except Exception:
    Credentials = None
    APIClient = None
    ModelInference = None

logger = logging.getLogger(__name__)
COVERAGE_SYS = """ë„ˆëŠ” í•œêµ­ ë³´í—˜ ì•½ê´€/ìš”ì•½ì„œ í…ìŠ¤íŠ¸ì—ì„œ ë³´ì¥ í•­ëª©ì„ êµ¬ì¡°í™”í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ë°˜ë“œì‹œ ë‚´ê°€ ì œê³µí•œ coverage_item ì´ë¦„ ëª©ë¡ ì¤‘ì—ì„œë§Œ nameì„ ì„ íƒí•´.
ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ í•­ëª© 1ê°œë¥¼ ì„ íƒí•˜ê³  notesì— "fuzzy: <ì›ë¬¸í‘œí˜„>"ì„ ë‚¨ê²¨.
segmentëŠ” ['ì…ì›','ì™¸ë˜','ì²˜ë°©','ì‘ê¸‰'] ì¤‘ ì„ íƒ, benefit_typeì€ ['ê¸‰ì—¬','ë¹„ê¸‰ì—¬','ì„ íƒ'] ì¤‘ ì„ íƒ.
ìˆ«ìëŠ” KRW ê¸°ì¤€ ì •ìˆ˜(ì›) ë˜ëŠ” ì†Œìˆ˜ì  2ìë¦¬. ë²”ìœ„/~ í‘œê¸°ëŠ” ë‹¨ì¼ ìˆ˜ì¹˜ë¡œ ì •ê·œí™”.
JSON ë°°ì—´ë§Œ ì¶œë ¥:
[
  {
    "name": "<coverage_item.name ì¤‘ í•˜ë‚˜>",
    "segment": "ì…ì›|ì™¸ë˜|ì²˜ë°©|ì‘ê¸‰",
    "benefit_type": "ê¸‰ì—¬|ë¹„ê¸‰ì—¬|ì„ íƒ",
    "coinsurance_pct": 0.20,
    "deductible_min": 10000,
    "per_visit_limit": 200000,
    "annual_limit": 50000000,
    "combined_cap_group": "ì½”ë“œ or null",
    "combined_cap_amount": 3500000,
    "frequency_limit": 50,
    "frequency_period": "year|contract|null",
    "coverage_order": 10,
    "notes": "ì„¤ëª…/ì£¼ì„",
    "source_ref": "ê·¼ê±° êµ¬ì ˆ/í˜ì´ì§€"
  }
]
"""

PREMIUM_SYS = """ë„ˆëŠ” í•œêµ­ ë³´í—˜ ì•½ê´€/ìš”ì•½ì„œì—ì„œ ë³´í—˜ë£Œ(ì›”ë³´í—˜ë£Œ) í‘œë¥¼ êµ¬ì¡°í™”í•œë‹¤.
ë¬¸ì„œì— ìˆ«ì ê¸ˆì•¡ì´ ëª…ì‹œëœ ê²½ìš°ë§Œ monthly_premium ì±„ìš´ë‹¤(ì—†ìœ¼ë©´ null).
tierëŠ” ë¬¸ì„œì˜ í”Œëœ ëª…ì¹­ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ ì—†ìœ¼ë©´ "standard".
genderëŠ” 'M','F','A' ì¤‘ í•˜ë‚˜. ì—°ë ¹ëŒ€ëŠ” í‘œì˜ êµ¬ê°„ì„ ê·¸ëŒ€ë¡œ [age_min, age_max].
JSON ë°°ì—´ë§Œ ì¶œë ¥:
[
  {
    "age_min": 20,
    "age_max": 29,
    "gender": "M",
    "smoker": null,
    "tier": "ê¸°ë³¸í˜•",
    "monthly_premium": 12340.00,
    "currency": "KRW",
    "meta": { "ê·¼ê±°": "í‘œ í˜ì´ì§€/êµ¬ì ˆ", "ë¹„ê³ ": "ê°±ì‹ í˜•/ì‚°ì¶œê·¼ê±° ë“±" }
  }
]
"""
# ============================= OpenSearch =============================

def _os_client() -> OpenSearch:
    region = getattr(settings, "OPENSEARCH_REGION", os.getenv("OPENSEARCH_REGION", "us-east-1"))
    auth = AWSV4SignerAuth(boto3.Session().get_credentials(), region, "es")
    host = getattr(settings, "OPENSEARCH_HOST", os.getenv("OPENSEARCH_HOST"))
    port = int(getattr(settings, "OPENSEARCH_PORT", os.getenv("OPENSEARCH_PORT", "443")))
    return OpenSearch(
        hosts=[{"host": host, "port": port}],
        http_auth=auth,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection,
    )

def _trim(s: str, n: int = 120) -> str:
    s = (s or "").replace("\n", " ").strip()
    return (s[:n] + "â€¦") if len(s) > n else s

def _search_snippets(query: str, k: int = 8, policy_id: Optional[str] = None) -> List[Dict[str, Any]]:
    index = getattr(settings, "OPENSEARCH_INDEX", None)

    if policy_id:
        filter_block = [{
            "bool": {
                "should": [
                    {"term": {"policy_id": policy_id}}
                ]
            }
        }]

        must_block: List[Dict[str, Any]] = []
        if (query or "").strip():
            must_block.append({
                "multi_match": {
                    "query": query,
                    "fields": ["content^2", "section_title", "embed_input", "text"],
                    "type": "best_fields",
                    "operator": "or",
                }
            })
        else:
            must_block.append({"match_all": {}})

        body: Dict[str, Any] = {
            "size": max(1, min(k, 20)),
            "query": {
                "bool": {
                    "filter": filter_block,
                    "must": must_block,
                }
            },
            "_source": True,
        }
    else:
        body = {
            "size": max(1, min(k, 20)),
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["section_title^2", "content"],
                    "type": "best_fields",
                    "tie_breaker": 0.2
                }
            },
            "_source": True,
        }

    resp = _os_client().search(index=index, body=body)

    # total íŒŒì‹±(ë²„ì „ì— ë”°ë¼ dict/int í˜¼ì¬)
    raw_total = (resp.get("hits", {}).get("total", {}) or {})
    if isinstance(raw_total, dict):
        total = int(raw_total.get("value", 0))
    else:
        total = int(raw_total or 0)

    hits = resp.get("hits", {}).get("hits", []) or []

    if not hits:
        # âœ… ê²€ìƒ‰ 0ê±´ ë¡œê·¸
        logger.info(
            "[RAG][OS_EMPTY] index=%s total=%d k=%d query=%s",
            index, total, k, _trim(query)
        )
        return []

    out = []
    for h in hits:
        s = h.get("_source", {}) or {}
        out.append({
            "score": h.get("_score", 0.0),
            "section_title": s.get("section_title", "") or "",
            "content": s.get("content", "") or "",
            "filename": s.get("filename", ""),
            "insurer": s.get("insurer", ""),
            "version": s.get("version", ""),
            "policy_id": s.get("policy_id", "") or s.get("policy", ""),
            "effective_date": s.get("effective_date", ""),
        })

    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë¡œê·¸(ìƒìœ„ 1ê°œë§Œ ì œëª© ì°ê¸°)
    logger.debug(
        "[RAG][OS_OK] index=%s total=%d k=%d top_title=%s query=%s",
        index, total, k, _trim(out[0].get("section_title")), _trim(query)
    )
    return out

# ============================= Token Utils =============================

def _rough_tokens(text: str) -> int:
    """ë³´ìˆ˜ ê·¼ì‚¬(í•œê¸€ í¬í•¨): ~1í† í°=3ë¬¸ì."""
    if not text:
        return 0
    return max(1, len(text) // 3)

def _wx_model() -> Optional[ModelInference]:
    if ModelInference is None:
        logger.warning("ibm-watsonx-ai SDK not installed.")
        return None

    creds = Credentials(api_key=settings.WATSONX_API_KEY, url=settings.WATSONX_URL)
    _ = APIClient(creds)  # ë‚´ë¶€ í† í¬ë‚˜ì´ì € í˜¸ì¶œì— í•„ìš”

    return ModelInference(
        model_id=settings.WATSONX_MODEL_ID,
        credentials=creds,
        space_id=settings.WATSONX_SPACE_ID,
    )

def _wx_token_count(model: Optional[ModelInference], text: str) -> int:
    if not model:
        return _rough_tokens(text)
    try:
        t = model.tokenize(prompt=text, return_tokens=True)
        return len(t.get("result", []))
    except Exception:
        return _rough_tokens(text)

def _fit_snippets_to_limit(
    snippets: List[Dict[str, Any]],
    user_query: str,
    token_limit: int = 30000,
    system_overhead_tokens: int = 600,
    per_snippet_header_tokens: int = 12,
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    watsonx í˜¸ì¶œ ì „, RAG ì»¨í…ìŠ¤íŠ¸(ì›ë¬¸ ìŠ¤ë‹ˆí«) ë¸”ë¡ì„ 10,000 í† í° ì´ë‚´ë¡œ êµ¬ì„±.
    watsonx í† í¬ë‚˜ì´ì € ì—†ì´ ë³´ìˆ˜ ê·¼ì‚¬ ì‚¬ìš©.
    """
    header = "[RAG CONTEXT]\n"
    used = _rough_tokens(header) + _rough_tokens(user_query) + system_overhead_tokens
    chosen: List[Dict[str, Any]] = []
    lines: List[str] = [header]

    for s in snippets:
        title = s.get("section_title") or "snippet"
        body = (s.get("content") or "").strip()
        if not body:
            continue
        block = f"- ({title}) {body}"
        need = per_snippet_header_tokens + _rough_tokens(block)

        if used + need > token_limit:
            remain = token_limit - used - per_snippet_header_tokens
            if remain <= 0:
                break
            approx_chars = max(0, remain * 3)
            block = f"- ({title}) {body[:approx_chars]}â€¦"
            need = per_snippet_header_tokens + _rough_tokens(block)
            if used + need > token_limit:
                break

        lines.append(block)
        used += need
        chosen.append(s)

    # ê°„ë‹¨ ì¶œì²˜
    if chosen:
        labels = []
        for s in chosen:
            parts = []
            if s.get("insurer"): parts.append(str(s["insurer"]))
            if s.get("version"): parts.append(str(s["version"]))
            if s.get("filename"): parts.append(str(s["filename"]))
            if s.get("policy_id"): parts.append(f"#{s['policy_id']}")
            lab = " ".join([p for p in parts if p])
            if lab and lab not in labels:
                labels.append(lab)
        if labels:
            lines.append("ğŸ” ì¶œì²˜: " + " / ".join(labels))

    return "\n".join(lines), chosen

# ============================= watsonx Generator =============================

def _rag_prompt(user_query: str, context_block: str) -> str:
    return (
        "ë‹¹ì‹ ì€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ë‹µí•˜ëŠ” ë³´í—˜ ë„ë©”ì¸ RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ê·œì¹™:\n"
        "1) ì»¨í…ìŠ¤íŠ¸ ë°– ì§€ì‹ ì‚¬ìš© ê¸ˆì§€.\n"
        "2) ê¸ˆì•¡/í•œë„/ë©´ì±… ë“± ìˆ˜ì¹˜ëŠ” ì¸ìš© ê·¼ê±°ì™€ í•¨ê»˜ ì œì‹œ.\n"
        "3) ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œ.\n\n"
        f"{context_block}\n\n"
        f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_query}\n\n"
        "ì¶œë ¥ í˜•ì‹: ìš”ì•½ â†’ ê·¼ê±°(ê¸€ë¨¸ë¦¬í‘œ) â†’ ìœ ì˜ì‚¬í•­ â†’ ì¶œì²˜"
    )

def _wx_generate_answer(prompt: str) -> str:
    model = _wx_model()
    if model is None:
        return ""
    try:
        gen_params = {
            GenParams.TEMPERATURE: 0.2,
            GenParams.DECODING_METHOD: "greedy",
            GenParams.REPETITION_PENALTY: 1.05,
            GenParams.STOP_SEQUENCES: [],
        }
        resp = model.generate_text(prompt=prompt, params=gen_params)

        return (resp or "").strip()
    except Exception as e:
        logger.exception("watsonx.ai generation failed: %s", e)
        return ""

# ============================= Public API =============================
def retrieve(
    *,
    mode,
    user_id: str,
    query: str,
    attachment_ids: List[str] | None = None,
    product_id: Optional[str] = None,
    limit: int = 5,
    fallback_to_global: bool = False,
) -> str:
    try:
        if product_id:
            snippets = _search_snippets(query=query, k=limit, policy_id=product_id)
        else:
            snippets = _search_snippets(query=query, k=limit)
        context_block, _ = _fit_snippets_to_limit(
            snippets=snippets,
            user_query=query,
        )
        prompt = _rag_prompt(user_query=query, context_block=context_block)
        print("[RAG AUTO PROMPT END]\n" + prompt)
        answer = _wx_generate_answer(prompt)
        print("[RAG AUTO ANSWER END]\n" + answer)
        if (answer or "").strip():
            # ì™¸ë¶€(OpenAI) í´ë¦¬ì‹±ì´ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëª…í™•í•œ í—¤ë” ë¶€ì—¬
            return "[RAG AUTO ANSWER]\n" + answer

        # watsonx ì‹¤íŒ¨ ì‹œ: ì›ë¬¸ ì»¨í…ìŠ¤íŠ¸ë¼ë„ ë°˜í™˜
        return context_block

    except Exception as e:
        logger.exception("retrieve failed: %s", e)
        return ""
#
from typing import Dict, Any

async def policy_db_lookup(*, mode: Mode, entities: Dict[str, Any], user_text: str) -> str:
    """
    TODO: ì‹¤ì œ ì•½ê´€ DB ì§ì¡°íšŒ ë¡œì§ìœ¼ë¡œ êµì²´.
    í˜„ì¬ëŠ” ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ì—¬ stageì—ì„œ RAG ë³´ì¡°ë¥¼ ì‹œë„í•˜ê²Œ ë‘¡ë‹ˆë‹¤.
    """
    try:
        return ""
    except Exception as e:
        logger.warning("policy_db_lookup failed: %s", e)
        return ""

# app/rag/retriever.py (í•˜ë‹¨ì— ì¶”ê°€)
from typing import List, Dict, Any, Tuple, Optional
import math, json
from starlette.concurrency import run_in_threadpool

MODEL_LIMIT = 20_000           # llama-4 maverick 17B ì…ë ¥ ì´ í•œë„
OUTPUT_RESERVE = 8_096          # ì¶œë ¥ í† í° ì˜ˆì•½ (ìƒí™©ì— ë”°ë¼ ì¡°ì ˆ)
PROMPT_OVERHEAD = 2_000         # ì‹œìŠ¤í…œ/ì§€ì‹œ/ë©”íƒ€ ì˜¤ë²„í—¤ë“œ ê°€ì •
CONTEXT_BUDGET = MODEL_LIMIT - OUTPUT_RESERVE - PROMPT_OVERHEAD

def _tok_count(s: str) -> int:
    # watson í† í¬ë‚˜ì´ì € ìˆìœ¼ë©´ ì •í™•, ì—†ìœ¼ë©´ ëŸ¬í”„
    m = _wx_model()
    try:
        return _wx_token_count(m, s)
    except Exception:
        return _rough_tokens(s)

def _chunk_to_text(c: Dict[str, Any]) -> str:
    title = (c.get("section_title") or "").strip()
    body  = (c.get("content") or c.get("embed_input") or c.get("text") or "").strip()
    if not body:
        return ""
    return f"### {title}\n{body}" if title else body

def _pack_batches_by_tokens(chunks: List[Dict[str, Any]], budget_tokens: int = CONTEXT_BUDGET) -> List[List[Dict[str, Any]]]:
    """
    ì¡°ê° ë¦¬ìŠ¤íŠ¸ë¥¼ í† í° ì˜ˆì‚°ì— ë§ì¶° ì—¬ëŸ¬ ë°°ì¹˜ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    """
    batches: List[List[Dict[str, Any]]] = []
    cur: List[Dict[str, Any]] = []
    used = 0
    for c in chunks:
        print("ì²­í¬ë¥¼ í† í° ì˜ˆì‚°ì—ì„œ ë§ì¶°ì„œ ë‹¤ì‹œ ì¬ë°°ì¹˜ë¡œ ë¶„í• ")
        t = _chunk_to_text(c)
        if not t:
            continue
        cost = _tok_count(t) + 24  # í—¤ë” ì—¬ìœ ì¹˜
        if cost > budget_tokens * 0.9:
            # ê°œë³„ ì¡°ê°ì´ ë„ˆë¬´ í° ê²½ìš°: ê³¼ê°íˆ ì˜ë¼ì„œ ì¤„ì´ê¸°
            short = t[: max(1, (budget_tokens // 3) * 3)]
            c = dict(c)
            c["content"] = short
            cost = _tok_count(_chunk_to_text(c)) + 24

        if used + cost > budget_tokens and cur:
            batches.append(cur); cur = []; used = 0
        cur.append(c); used += cost

    if cur:
        batches.append(cur)
    return batches

async def _wx_async(prompt: str) -> str:
    return await run_in_threadpool(_wx_generate_answer, prompt)

def extract_json_array(text: str) -> List[Any]:
    try:
        obj = json.loads(text)
        return obj if isinstance(obj, list) else []
    except Exception:
        i, j = text.find("["), text.rfind("]")
        if i >= 0 and j >= i:
            try:
                return json.loads(text[i:j+1])
            except Exception:
                return []
        return []

# ----------------------- MAP (Watson í˜¸ì¶œ) -----------------------

async def extract_coverage_batched(base_names: List[str], chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    batches = _pack_batches_by_tokens(chunks)
    out: List[Dict[str, Any]] = []
    for b in batches:
        print("========WATSON BATCH í˜¸ì¶œ==========")
        prompt = _coverage_prompt(base_names, b)   # â† í”„ë¡¬í”„íŠ¸ëŠ” ë¬¸ìì—´
        txt = await _wx_async(prompt)
        arr = extract_json_array(txt)
        # JSON íƒ€ì… ë°©ì–´ (ìˆ«ì ë¬¸ìì—´ -> ìˆ«ì)
        for it in arr:
            if isinstance(it, dict):
                for k in ("coinsurance_pct","deductible_min","per_visit_limit","annual_limit","combined_cap_amount","frequency_limit","coverage_order"):
                    if k in it and isinstance(it[k], str):
                        try:
                            it[k] = float(it[k]) if "." in it[k] else int(it[k])
                        except Exception:
                            pass
        out.extend([x for x in arr if isinstance(x, dict)])
    return out

async def extract_premiums_batched(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    batches = _pack_batches_by_tokens(chunks)
    out: List[Dict[str, Any]] = []
    for b in batches:
        prompt = _premium_prompt(b)
        txt = await _wx_async(prompt)
        arr = extract_json_array(txt)
        for it in arr:
            if isinstance(it, dict):
                for k in ("age_min","age_max"):
                    if k in it and isinstance(it[k], str):
                        try: it[k] = int(it[k])
                        except: pass
                if "monthly_premium" in it and isinstance(it["monthly_premium"], str):
                    try: it["monthly_premium"] = float(it["monthly_premium"])
                    except: pass
        out.extend([x for x in arr if isinstance(x, dict)])
    return out

# ----------------------- REDUCE (ì„œë²„ ë³‘í•©) -----------------------

def _merge_num(a, b, mode: str):
    to_num = lambda x: None if x in (None, "", "null") else x
    a, b = to_num(a), to_num(b)
    if a is None: return b
    if b is None: return a
    if mode == "min": return a if a <= b else b
    if mode == "max": return a if a >= b else b
    return a  # default keep

def _concat_notes(*vals: Optional[str]) -> str:
    parts = [v for v in vals if isinstance(v, str) and v.strip()]
    # ì¤‘ë³µ/ê¸¸ì´ ë°©ì§€
    seen, out = set(), []
    for p in parts:
        if p not in seen:
            out.append(p); seen.add(p)
    return " | ".join(out)[:2000]

def reduce_coverage(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    (name, segment, benefit_type, combined_cap_group)ë¡œ ê·¸ë£¹í•‘í•˜ê³ 
    í•œë„ë¥˜ëŠ” max, ê³µì œ/ë³¸ì¸ë¶€ë‹´ì€ minì„ ì·¨í•¨. ì¶©ëŒì€ notesì— ê¸°ë¡.
    """
    key = lambda d: (
        d.get("name"),
        d.get("segment"),
        d.get("benefit_type"),
        d.get("combined_cap_group"),
    )
    agg: Dict[Tuple, Dict[str, Any]] = {}
    for it in items:
        k = key(it)
        if k not in agg:
            agg[k] = dict(it)
            continue
        cur = agg[k]
        # ê·œì¹™ ë³‘í•©
        cur["coinsurance_pct"] = _merge_num(cur.get("coinsurance_pct"), it.get("coinsurance_pct"), "min")
        cur["deductible_min"]  = _merge_num(cur.get("deductible_min"),  it.get("deductible_min"),  "min")
        cur["per_visit_limit"] = _merge_num(cur.get("per_visit_limit"), it.get("per_visit_limit"), "max")
        cur["annual_limit"]    = _merge_num(cur.get("annual_limit"),    it.get("annual_limit"),    "max")
        cur["combined_cap_amount"] = _merge_num(cur.get("combined_cap_amount"), it.get("combined_cap_amount"), "max")
        cur["frequency_limit"] = _merge_num(cur.get("frequency_limit"), it.get("frequency_limit"), "max")

        # coverage_orderëŠ” ê°€ì¥ ì‘ì€(ìš°ì„ ìˆœìœ„ ë†’ì€) ê°’ ìœ ì§€
        cur["coverage_order"]  = _merge_num(cur.get("coverage_order"), it.get("coverage_order"), "min")

        # source_ref, notesëŠ” í•©ì¹˜ê¸°
        cur["source_ref"] = _concat_notes(cur.get("source_ref"), it.get("source_ref"))
        cur["notes"]      = _concat_notes(cur.get("notes"), it.get("notes"))

        # currency/period ë“± ëˆ„ë½ ë³´ì™„
        for f in ("frequency_period", "combined_cap_group"):
            if not cur.get(f) and it.get(f):
                cur[f] = it[f]
    # ë¦¬ìŠ¤íŠ¸ ë°˜í™˜(ì •ë ¬ì€ coverage_order â†’ name)
    return sorted(agg.values(), key=lambda d: (d.get("coverage_order") or 9999, str(d.get("name") or "")))

def reduce_premiums(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    (age_min, age_max, gender, smoker, tier)ë¡œ ê·¸ë£¹í•‘, ì›”ë³´í—˜ë£ŒëŠ” ê°’ ë‹¤ë¥´ë©´ í‰ê· /ìµœì†Ÿê°’ ì¤‘ íƒ1.
    ì—¬ê¸°ì„œëŠ” ë¬¸ì„œ ìƒ ë‹¨ì¼í‘œ ê¸°ì¤€ìœ¼ë¡œ minì„ ìœ ì§€(ë³´ìˆ˜ì ).
    """
    key = lambda d: (d.get("age_min"), d.get("age_max"), d.get("gender"), d.get("smoker"), d.get("tier") or "standard")
    agg: Dict[Tuple, Dict[str, Any]] = {}
    for it in items:
        k = key(it)
        if k not in agg:
            agg[k] = dict(it)
            continue
        cur = agg[k]
        # ê¸ˆì•¡ ì¶©ëŒ ì‹œ ë” ì‘ì€ ê°’ ìœ ì§€(ë³´ìˆ˜ì )
        if it.get("monthly_premium") is not None:
            if cur.get("monthly_premium") is None:
                cur["monthly_premium"] = it["monthly_premium"]
            else:
                cur["monthly_premium"] = min(cur["monthly_premium"], it["monthly_premium"])
        # ê·¼ê±°/ë¹„ê³  í•©ì¹˜ê¸°
        meta = cur.get("meta") or {}
        meta2 = it.get("meta") or {}
        note = _concat_notes(
            (meta.get("ë¹„ê³ ") if isinstance(meta.get("ë¹„ê³ "), str) else None),
            (meta2.get("ë¹„ê³ ") if isinstance(meta2.get("ë¹„ê³ "), str) else None),
        )
        base = _concat_notes(
            (meta.get("ê·¼ê±°") if isinstance(meta.get("ê·¼ê±°"), str) else None),
            (meta2.get("ê·¼ê±°") if isinstance(meta2.get("ê·¼ê±°"), str) else None),
        )
        cur["meta"] = {"ê·¼ê±°": base, "ë¹„ê³ ": note}
        # currency ê¸°ë³¸ê°’
        if not cur.get("currency"):
            cur["currency"] = it.get("currency") or "KRW"

    return sorted(agg.values(), key=lambda d: (d.get("tier") or "standard", d.get("gender") or "A", d.get("age_min") or 0))

def _chunks_to_context(chunks: List[Dict[str, Any]], max_chars: int = 500_000) -> str:
    """section_title, content/embed_input/text ìˆœìœ¼ë¡œ ë¬¸ë§¥ í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±° í¬í•¨)."""
    parts: List[str] = []
    for c in chunks:
        title = (c.get("section_title") or "").strip()
        body = (c.get("content") or c.get("embed_input") or c.get("text") or "").strip()
        if not body:
            continue
        if title:
            parts.append(f"### {title}\n{body}")
        else:
            parts.append(body)
    # ì¤‘ë³µ ì œê±°
    seen, uniq = set(), []
    for p in parts:
        if p not in seen:
            uniq.append(p); seen.add(p)
    ctx = "\n\n".join(uniq)
    return ctx[:max_chars]

def _coverage_prompt(base_names: List[str], chunks: List[Dict[str, Any]]) -> str:
    names_json = json.dumps(base_names, ensure_ascii=False)
    context = _chunks_to_context(chunks)
    return (
        f"{COVERAGE_SYS}\n\n"
        f"[coverage_item.name ëª©ë¡]\n{names_json}\n\n"
        f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
        "[ì§€ì‹œì‚¬í•­]\n"
        "- JSON ë°°ì—´ë§Œ ì¶œë ¥. ì£¼ì„/ì„¤ëª…/ë¼ë²¨ ê¸ˆì§€.\n"
        "- ìˆ˜ì¹˜ëŠ” KRW ë‹¨ìœ„. ë²”ìœ„/â€˜~â€™ í‘œê¸°ëŠ” ë‹¨ì¼ ìˆ˜ì¹˜ë¡œ ì •ê·œí™”.\n"
        "- ê° í•­ëª©ì— source_refë¡œ ê·¼ê±° êµ¬ì ˆ/í˜ì´ì§€ë¥¼ ë‚¨ê²¨."
    )

def _premium_prompt(chunks: List[Dict[str, Any]]) -> str:
    context = _chunks_to_context(chunks)
    return (
        f"{PREMIUM_SYS}\n\n"
        f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
        "[ì§€ì‹œì‚¬í•­]\n"
        "- JSON ë°°ì—´ë§Œ ì¶œë ¥. ì£¼ì„/ì„¤ëª…/ë¼ë²¨ ê¸ˆì§€.\n"
        "- ë¬¸ì„œì— ì‹¤ì œ ê¸ˆì•¡ ìˆì„ ë•Œë§Œ monthly_premium ì±„ì›€."
    )
