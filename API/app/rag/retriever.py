# app/rag/retriever.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
import os
import logging
import boto3
import json, re
from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth
from app.services.common import Mode
from app.config import settings
from typing import List, Dict, Any
from starlette.concurrency import run_in_threadpool
from sqlalchemy import select, func
from app.database import AsyncSessionLocal
from app.models import (
    InsurancePolicy,
    PolicyCoverage,
    CoverageItemWeight,
    PolicyPremium,
    ComplementarityRules,
    CoverageItem,
)
from typing import List, Optional
from sqlalchemy import MetaData, Table, select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload
# watsonx.ai (pip install ibm-watsonx-ai)
try:
    from ibm_watsonx_ai import Credentials, APIClient
    from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
    from ibm_watsonx_ai.foundation_models import ModelInference
except Exception:
    Credentials = None
    APIClient = None
    ModelInference = None

logger = logging.getLogger(__name__)
COVERAGE_SYS = """ë„ˆëŠ” í•œêµ­ ë³´í—˜ ì•½ê´€/ìš”ì•½ì„œ í…ìŠ¤íŠ¸ì—ì„œ ë³´ì¥ í•­ëª©ì„ êµ¬ì¡°í™”í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ë°˜ë“œì‹œ ë‚´ê°€ ì œê³µí•œ coverage_item ì´ë¦„ ëª©ë¡ ì¤‘ì—ì„œë§Œ nameì„ ì„ íƒí•´.
ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ í•­ëª© 1ê°œë¥¼ ì„ íƒí•˜ê³  notesì— "fuzzy: <ì›ë¬¸í‘œí˜„>"ì„ ë‚¨ê²¨.
segmentëŠ” ['ì…ì›','ì™¸ë˜','ì²˜ë°©','ì‘ê¸‰'] ì¤‘ ì„ íƒ, benefit_typeì€ ['ê¸‰ì—¬','ë¹„ê¸‰ì—¬','ì„ íƒ'] ì¤‘ ì„ íƒ.
ìˆ«ìëŠ” KRW ê¸°ì¤€ ì •ìˆ˜(ì›) ë˜ëŠ” ì†Œìˆ˜ì  2ìë¦¬. ë²”ìœ„/~ í‘œê¸°ëŠ” ë‹¨ì¼ ìˆ˜ì¹˜ë¡œ ì •ê·œí™”.
coverage_orderëŠ” intí˜•(ì •ìˆ˜)ë¡œ ë¬¸ì„œì˜ UI/ì„¤ëª…ì„ í‘œì‹œí•˜ëŠ” ìˆœì„œ.
JSON ë°°ì—´ë§Œ ì¶œë ¥:
[
  {
    "name": "<coverage_item.name ì¤‘ í•˜ë‚˜>",
    "segment": "ì…ì›|ì™¸ë˜|ì²˜ë°©|ì‘ê¸‰",
    "benefit_type": "ê¸‰ì—¬|ë¹„ê¸‰ì—¬|ì„ íƒ",
    "coinsurance_pct": 0.20,
    "deductible_min": 10000,
    "per_visit_limit": 200000,
    "annual_limit": 50000000,
    "combined_cap_group": "ì½”ë“œ or null",
    "combined_cap_amount": 3500000,
    "frequency_limit": 50,
    "frequency_period": "year|contract|null",
    "coverage_order": 1,
    "notes": "ì„¤ëª…/ì£¼ì„",
    "source_ref": "ê·¼ê±° êµ¬ì ˆ/í˜ì´ì§€"
  }
]
"""

PREMIUM_SYS = """ë„ˆëŠ” í•œêµ­ ë³´í—˜ ì•½ê´€/ìš”ì•½ì„œì—ì„œ ë³´í—˜ë£Œ(ì›”ë³´í—˜ë£Œ) í‘œë¥¼ êµ¬ì¡°í™”í•œë‹¤.
ë¬¸ì„œì— ìˆ«ì ê¸ˆì•¡ì´ ëª…ì‹œëœ ê²½ìš°ë§Œ monthly_premium ì±„ìš´ë‹¤(ì—†ìœ¼ë©´ null).
tierëŠ” ë¬¸ì„œì˜ í”Œëœ ëª…ì¹­ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ ì—†ìœ¼ë©´ "standard".
smokerëŠ” BOOLEANê°’ìœ¼ë¡œ ë¦¬í„´ TRUE,FALSE
genderëŠ” 'M','F','A' ì¤‘ í•˜ë‚˜. ì—°ë ¹ëŒ€ëŠ” í‘œì˜ êµ¬ê°„ì„ ê·¸ëŒ€ë¡œ [age_min, age_max].
JSON ë°°ì—´ë§Œ ì¶œë ¥:
[
  {
    "age_min": 20,
    "age_max": 29,
    "gender": "M",
    "smoker": null,
    "tier": "ê¸°ë³¸í˜•",
    "monthly_premium": 12340.00,
    "currency": "KRW",
    "meta": { "ê·¼ê±°": "í‘œ í˜ì´ì§€/êµ¬ì ˆ", "ë¹„ê³ ": "ê°±ì‹ í˜•/ì‚°ì¶œê·¼ê±° ë“±" }
  }
]
"""

POLICY_META_SYS = """ë„ˆëŠ” í•œêµ­ ë³´í—˜ ì•½ê´€/ìš”ì•½ì„œì—ì„œ ìƒí’ˆ ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
'ì¶œë ¥ì€ ì˜¤ì§ JSON ë°°ì—´ í•˜ë‚˜. í‚¤ëŠ” ì •í™•íˆ ë‹¤ìŒë§Œ ì‚¬ìš©(ëŒ€ì†Œë¬¸ì í¬í•¨, ì¶”ê°€/ëˆ„ë½/ë‹¤ë¥¸ ì¼€ì´ìŠ¤ ê¸ˆì§€): '
'["product_type","renewal_type","waiting_period_days","age_min","age_max","gender_allowed","is_catalog","attrs"]'
JSON ë°°ì—´ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ê³ , ê°ì²´ëŠ” ë‹¤ìŒ í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•´:
{
  "product_type": "ì•”ë³´í—˜|ì‹¤ì†|ìš´ì „ì|ì¢…ì‹ |ì •ê¸°|ì¹˜ì•„|ì–´ë¦°ì´|ê°„ë³‘|ê¸°íƒ€",
  "renewal_type": "ë¹„ê°±ì‹ |ì—°ë§Œê¸°ê°±ì‹ |3ë…„ê°±ì‹ |5ë…„ê°±ì‹ |ê¸°íƒ€",
  "waiting_period_days": 90,
  "age_min": 0,
  "age_max": 100,
  "gender_allowed": "M|F|A",
  "is_catalog": false,
  "attrs": {"í‚¤": "ê°’"}
}
ê°’ì´ ì—†ìœ¼ë©´ nullë¡œ ë‚¨ê²¨.
"""

# ============================= OpenSearch =============================

def _os_client() -> OpenSearch:
    region = getattr(settings, "OPENSEARCH_REGION", os.getenv("OPENSEARCH_REGION", "us-east-1"))
    auth = AWSV4SignerAuth(boto3.Session().get_credentials(), region, "es")
    host = getattr(settings, "OPENSEARCH_HOST", os.getenv("OPENSEARCH_HOST"))
    port = int(getattr(settings, "OPENSEARCH_PORT", os.getenv("OPENSEARCH_PORT", "443")))
    return OpenSearch(
        hosts=[{"host": host, "port": port}],
        http_auth=auth,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection,
        timeout=30,
    )

def _trim(s: str, n: int = 120) -> str:
    s = (s or "").replace("\n", " ").strip()
    return (s[:n] + "â€¦") if len(s) > n else s

def _search_snippets(query: str, k: int = 8, policy_id: Optional[str] = None, policy_ids: Optional[List[str]] = None,) -> List[Dict[str, Any]]:
    index = getattr(settings, "OPENSEARCH_INDEX", None)

    must_block: List[Dict[str, Any]] = []
    if (query or "").strip():
        must_block.append(
            {
                "multi_match": {
                    "query": query,
                    "fields": ["content^2", "embed_input", "text"],
                    "type": "best_fields",
                    "operator": "or",
                }
            }
        )
    else:
        must_block.append({"match_all": {}})

    filter_block = []
    if policy_id:
        filter_block.append({"term": {"policy_id": policy_id}})
    elif policy_ids:
        filter_block.append({"terms": {"policy_id": policy_ids}})

    if policy_id or policy_ids:
        body: Dict[str, Any] = {
            "size": max(1, min(k, 20)),
            "query": {"bool": {"filter": filter_block, "must": must_block}},
            "_source": True,
        }
    else:
        body = {
            "size": max(1, min(k, 20)),
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["section_title^2", "content"],
                    "type": "best_fields",
                    "tie_breaker": 0.2,
                }
            },
            "_source": True,
        }

    resp = _os_client().search(index=index, body=body)

    # total íŒŒì‹±(ë²„ì „ì— ë”°ë¼ dict/int í˜¼ì¬)
    raw_total = (resp.get("hits", {}).get("total", {}) or {})
    if isinstance(raw_total, dict):
        total = int(raw_total.get("value", 0))
    else:
        total = int(raw_total or 0)

    hits = resp.get("hits", {}).get("hits", []) or []

    if not hits:
        # âœ… ê²€ìƒ‰ 0ê±´ ë¡œê·¸
        logger.info(
            "[RAG][OS_EMPTY] index=%s total=%d k=%d query=%s",
            index, total, k, _trim(query)
        )
        return []

    out = []
    for h in hits:
        s = h.get("_source", {}) or {}
        out.append({
            "score": h.get("_score", 0.0),
            "section_title": s.get("section_title", "") or "",
            "content": s.get("content", "") or "",
            "filename": s.get("filename", ""),
            "insurer": s.get("insurer", ""),
            "version": s.get("version", ""),
            "policy_id": s.get("policy_id", "") or s.get("policy", ""),
            "effective_date": s.get("effective_date", ""),
        })
    print(out)
    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë¡œê·¸(ìƒìœ„ 1ê°œë§Œ ì œëª© ì°ê¸°)
    logger.debug(
        "[RAG][OS_OK] index=%s total=%d k=%d top_title=%s query=%s",
        index, total, k, _trim(out[0].get("section_title")), _trim(query)
    )
    return out

# ============================= Token Utils =============================

def _rough_tokens(text: str) -> int:
    """ë³´ìˆ˜ ê·¼ì‚¬(í•œê¸€ í¬í•¨): ~1í† í°=3ë¬¸ì."""
    if not text:
        return 0
    return max(1, len(text) // 3)

def _wx_model() -> Optional[ModelInference]:
    if ModelInference is None:
        logger.warning("ibm-watsonx-ai SDK not installed.")
        return None

    creds = Credentials(api_key=settings.WATSONX_API_KEY, url=settings.WATSONX_URL)
    _ = APIClient(creds)  # ë‚´ë¶€ í† í¬ë‚˜ì´ì € í˜¸ì¶œì— í•„ìš”

    return ModelInference(
        model_id=settings.WATSONX_MODEL_ID,
        credentials=creds,
        space_id=settings.WATSONX_SPACE_ID,
    )

def _wx_token_count(model: Optional[ModelInference], text: str) -> int:
    if not model:
        return _rough_tokens(text)
    try:
        t = model.tokenize(prompt=text, return_tokens=True)
        return len(t.get("result", []))
    except Exception:
        return _rough_tokens(text)

def _fit_snippets_to_limit(
    snippets: List[Dict[str, Any]],
    user_query: str,
    token_limit: int = 30000,
    system_overhead_tokens: int = 600,
    per_snippet_header_tokens: int = 12,
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    watsonx í˜¸ì¶œ ì „, RAG ì»¨í…ìŠ¤íŠ¸(ì›ë¬¸ ìŠ¤ë‹ˆí«) ë¸”ë¡ì„ 10,000 í† í° ì´ë‚´ë¡œ êµ¬ì„±.
    watsonx í† í¬ë‚˜ì´ì € ì—†ì´ ë³´ìˆ˜ ê·¼ì‚¬ ì‚¬ìš©.
    """
    header = "[RAG CONTEXT]\n"
    used = _rough_tokens(header) + _rough_tokens(user_query) + system_overhead_tokens
    chosen: List[Dict[str, Any]] = []
    lines: List[str] = [header]

    for s in snippets:
        title = s.get("section_title") or "snippet"
        body = (s.get("content") or "").strip()
        if not body:
            continue
        block = f"- ({title}) {body}"
        need = per_snippet_header_tokens + _rough_tokens(block)

        if used + need > token_limit:
            remain = token_limit - used - per_snippet_header_tokens
            if remain <= 0:
                break
            approx_chars = max(0, remain * 3)
            block = f"- ({title}) {body[:approx_chars]}â€¦"
            need = per_snippet_header_tokens + _rough_tokens(block)
            if used + need > token_limit:
                break

        lines.append(block)
        used += need
        chosen.append(s)

    # ê°„ë‹¨ ì¶œì²˜
    if chosen:
        labels = []
        for s in chosen:
            parts = []
            if s.get("insurer"): parts.append(str(s["insurer"]))
            if s.get("version"): parts.append(str(s["version"]))
            if s.get("filename"): parts.append(str(s["filename"]))
            if s.get("policy_id"): parts.append(f"#{s['policy_id']}")
            lab = " ".join([p for p in parts if p])
            if lab and lab not in labels:
                labels.append(lab)
        if labels:
            lines.append("ğŸ” ì¶œì²˜: " + " / ".join(labels))

    return "\n".join(lines), chosen

# ============================= watsonx Generator =============================

def _rag_prompt(user_query: str, context_block: str) -> str:
    return (
        "ë‹¹ì‹ ì€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ë‹µí•˜ëŠ” ë³´í—˜ ë„ë©”ì¸ RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¨ê³„ë³„ ì„¤ëª…ì€ ìƒëµí•˜ê³  ì¶œë ¥ í˜•ì‹ì˜ ë‚´ìš©ì„ ìì„¸í•˜ê²Œ ì ì–´ì¤˜\n"
        "ê·œì¹™:\n"
        "1) ì»¨í…ìŠ¤íŠ¸ ë°– ì§€ì‹ ì‚¬ìš© ê¸ˆì§€.\n"
        "2) ê¸ˆì•¡/í•œë„/ë©´ì±… ë“± ìˆ˜ì¹˜ëŠ” ì¸ìš© ê·¼ê±°ì™€ í•¨ê»˜ ì œì‹œ.\n"
        "3) ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œ.\n\n"
        f"{context_block}\n\n"
        f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_query}\n\n"
        "ì¶œë ¥ í˜•ì‹: ìš”ì•½ â†’ ê·¼ê±°(ê¸€ë¨¸ë¦¬í‘œ) â†’ ìœ ì˜ì‚¬í•­ â†’ ì¶œì²˜"
    )

def _wx_generate_answer(prompt: str) -> str:
    model = _wx_model()
    if model is None:
        return ""
    try:
        gen_params = {
            GenParams.MAX_NEW_TOKENS: 8000,
            GenParams.TEMPERATURE: 0.2,
            GenParams.DECODING_METHOD: "greedy",
            GenParams.REPETITION_PENALTY: 1.05,
            GenParams.STOP_SEQUENCES: [],
        }
        resp = model.generate_text(prompt=prompt, params=gen_params)
        return (resp or "").strip()
    except Exception as e:
        logger.exception("watsonx.ai generation failed: %s", e)
        return ""

# ============================= Public API =============================
async def retrieve(
    *,
    mode,
    user_id: str,
    query: str,
    prev_chats: List[str] | None = None,
    product_id: Optional[str] = None,
    limit: int = 20,
    fallback_to_global: bool = False,
    db_context: str | None = None,
) -> str:
    try:
        snippets: List[Dict[str, Any]] = []
        if product_id:
            snippets = _search_snippets(query=query, k=limit, policy_id=product_id)
        elif user_id:
            user_id_int = int(user_id) if user_id and user_id.isdigit() else 0
            policy_ids: List[str] = []
            if user_id_int:
                async with AsyncSessionLocal() as session:
                    res = await session.execute(
                        select(InsurancePolicy.policy_id).where(
                            InsurancePolicy.user_id == user_id_int
                        )
                    )
                    policy_ids = [pid for pid in res.scalars().all() if pid]

            if policy_ids:
                snippets = _search_snippets(
                    query=query, k=limit, policy_ids=policy_ids
                )
            elif fallback_to_global:
                snippets = _search_snippets(query=query, k=limit)
        else:
            snippets = _search_snippets(query=query, k=limit)

        context_block, _ = _fit_snippets_to_limit(
            snippets=snippets,
            user_query=query,
        )
        prompt = _rag_prompt(
            user_query=query, context_block=context_block + (db_context or "")
        )
        print("[RAG AUTO PROMPT END]\n" + prompt)
        answer = await _wx_async(prompt)
        print("[RAG AUTO ANSWER END]\n" + answer)
        if (answer or "").strip():
            # ì™¸ë¶€(OpenAI) í´ë¦¬ì‹±ì´ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëª…í™•í•œ í—¤ë” ë¶€ì—¬
            return "[RAG AUTO ANSWER]\n" + answer

        # watsonx ì‹¤íŒ¨ ì‹œ: ì›ë¬¸ ì»¨í…ìŠ¤íŠ¸ë¼ë„ ë°˜í™˜
        return context_block

    except Exception as e:
        logger.exception("retrieve failed: %s", e)
        return ""
#
from typing import Dict, Any

async def policy_db_lookup(*, mode: Mode, entities: Dict[str, Any], user_text: str, user_id: int) -> str:
    """DB lookups for recommendation or refund modes."""
    try:
        async with AsyncSessionLocal() as session:
            if mode == Mode.RECOMMEND:
                # ë³´ìœ  ë³´í—˜ ì¡°íšŒ
                res = await session.execute(
                    select(InsurancePolicy).where(InsurancePolicy.user_id == user_id)
                )
                user_pols = res.scalars().all()
                if not user_pols:
                    return ""

                # ì¶”ì²œ í›„ë³´ (user_id=1)
                res = await session.execute(
                    select(InsurancePolicy).where(InsurancePolicy.user_id == 1)
                )
                candidates = res.scalars().all()
                if not candidates:
                    return ""

                # í›„ë³´ ë³´í—˜ë£Œ í‰ê·  ê³„ì‚°
                prem_res = await session.execute(
                    select(
                        PolicyPremium.policy_id,
                        func.avg(PolicyPremium.monthly_premium).label("avg_prem"),
                    )
                    .where(PolicyPremium.policy_id.in_([p.id for p in candidates]))
                    .group_by(PolicyPremium.policy_id)
                )
                avg_prem = {pid: float(v) for pid, v in prem_res}
                min_prem = min(avg_prem.values()) if avg_prem else 0.0
                max_prem = max(avg_prem.values()) if avg_prem else 0.0

                target_type = entities.get("product_type")
                recs = []
                print(candidates)
                for cand in candidates:
                    # ì í•©ë„(Fit)
                    fit = 0.0
                    if target_type and cand.product_type == target_type:
                        fit = 0.6

                    # ë³´ì¥(Coverage)
                    cov_res = await session.execute(
                        select(CoverageItemWeight.weight).join(
                            PolicyCoverage,
                            CoverageItemWeight.coverage_item_id
                            == PolicyCoverage.coverage_item_id,
                        ).where(
                            PolicyCoverage.policy_id == cand.id,
                            CoverageItemWeight.product_type == cand.product_type,
                        )
                    )
                    weights = [float(w) for (w,) in cov_res]
                    coverage = sum(weights) / len(weights) if weights else 0.0

                    # ê°€ê²©(Price)
                    p = avg_prem.get(cand.id)
                    if p is not None and max_prem > min_prem:
                        price = (max_prem - p) / (max_prem - min_prem)
                    else:
                        price = 0.5

                    # TODO : ì¶”í›„ì— ë³´í—˜ ì‹ ìš©ë„/ì¸ì§€ë„ ì ìˆ˜  ì¶”ê°€í•´ì„œ ê°€ì¤‘ì¹˜ ì ìš© í˜„ì¬ 0.5 ê³ ì •
                    trust = 0.5  # ê³ ì •ê°’

                    # ë³´ì™„ì„±(Complementarity)
                    comps: List[float] = []
                    for up in user_pols:
                        c_res = await session.execute(
                            select(ComplementarityRules.effect).where(
                                ComplementarityRules.src_product_type == up.product_type,
                                ComplementarityRules.dst_product_type == cand.product_type,
                            )
                        )
                        eff = c_res.scalar()
                        if eff is not None:
                            comps.append(float(eff))
                    comp_raw = sum(comps) / len(comps) if comps else 0.0
                    complementarity = (comp_raw + 1) / 2

                    score = (fit + coverage + price + trust + complementarity) / 5
                    recs.append(
                        {
                            "policy": cand,
                            "score": score,
                            "fit": fit,
                            "coverage": coverage,
                            "price": price,
                            "trust": trust,
                            "complementarity": complementarity,
                        }
                    )

                recs.sort(key=lambda r: r["score"], reverse=True)
                top = recs[:3]
                lines = ["[DB RECOMMENDATION]"]
                for idx, r in enumerate(top, 1):
                    pol = r["policy"]
                    ptype = getattr(pol.product_type, "value", pol.product_type) or ""
                    lines.append(
                        f"{idx}. {ptype} ({pol.policy_id or ''}) score {r['score']:.2f} - "
                        f"Fit {r['fit']:.2f}, Coverage {r['coverage']:.2f}, Price {r['price']:.2f}, "
                        f"Trust {r['trust']:.2f}, Complementarity {r['complementarity']:.2f}"
                    )
                return "\n".join(lines)
            elif mode == Mode.REFUND:
                res = await session.execute(
                    select(InsurancePolicy.policy_id).where(InsurancePolicy.user_id == user_id)
                )
                pol_id = res.scalar()
                if pol_id is None:
                    return ""
                res = await session.execute(
                    select(InsurancePolicy.id).where(InsurancePolicy.policy_id == pol_id).order_by(InsurancePolicy.id)
                )
                policy_id = res.scalar()
                if policy_id is None:
                    return ""

                res = await session.execute(
                    select(PolicyCoverage)
                    .options(selectinload(PolicyCoverage.coverage_item))
                    .where(PolicyCoverage.policy_id == policy_id)  # pol.idë¥¼ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©
                    .order_by(PolicyCoverage.coverage_order)
                )
                covs = res.scalars().all()
                if not covs:
                    return ""
                lines = ["[DB COVERAGE]"]
                for cov in covs:
                    name = getattr(cov.coverage_item, "name", "") or ""
                    parts: List[str] = []
                    if cov.segment:
                        parts.append(cov.segment)
                    if cov.benefit_type:
                        parts.append(cov.benefit_type)
                    if cov.coinsurance_pct is not None:
                        parts.append(f"ìê¸°ë¶€ë‹´ {float(cov.coinsurance_pct) * 100:.0f}%")
                    if cov.deductible_min is not None:
                        parts.append(f"ê³µì œ {int(cov.deductible_min):,}ì›")
                    if cov.per_visit_limit is not None:
                        parts.append(f"1íšŒ {int(cov.per_visit_limit):,}ì›")
                    if cov.annual_limit is not None:
                        parts.append(f"ì—°ê°„ {int(cov.annual_limit):,}ì›")
                    if cov.frequency_limit is not None:
                        period = "ì—°" if cov.frequency_period == "year" else "ê³„ì•½"
                        parts.append(f"íšŸìˆ˜ {cov.frequency_limit}íšŒ/{period}")

                    detail = ", ".join(parts)

                    ref = f" [ê·¼ê±°: {', '.join(cov.source_ref)}]" if cov.source_ref else ""

                    lines.append(f"- {pol_id}: {detail}{ref}")

                return "\n".join(lines)
            else:
                return ""
    except Exception as e:
        logger.warning("policy_db_lookup failed: %s", e)
        return ""

# app/rag/retriever.py (í•˜ë‹¨ì— ì¶”ê°€)
from typing import List, Dict, Any, Tuple, Optional
import math, json
from starlette.concurrency import run_in_threadpool

MODEL_LIMIT = 91_072           # llama-4 maverick 17B ì´ ì…ë ¥ í•œë„
OUTPUT_RESERVE = 8_096          # ì¶œë ¥ í† í° ì˜ˆì•½ (ìƒí™©ì— ë”°ë¼ ì¡°ì ˆ)
PROMPT_OVERHEAD = 2_000         # ì‹œìŠ¤í…œ/ì§€ì‹œ/ë©”íƒ€ ì˜¤ë²„í—¤ë“œ ê°€ì •
CONTEXT_BUDGET = 20_000         # ë¬¸ë§¥ ì²­í¬ì— í—ˆìš©í•  í† í° ìˆ˜
NAME_BUDGET = MODEL_LIMIT - OUTPUT_RESERVE - PROMPT_OVERHEAD - CONTEXT_BUDGET

def _tok_count(s: str) -> int:
    # watson í† í¬ë‚˜ì´ì € ìˆìœ¼ë©´ ì •í™•, ì—†ìœ¼ë©´ ëŸ¬í”„
    m = _wx_model()
    try:
        return _wx_token_count(m, s)
    except Exception:
        return _rough_tokens(s)

def _chunk_to_text(c: Dict[str, Any]) -> str:
    title = (c.get("section_title") or "").strip()
    body  = (c.get("content") or c.get("embed_input") or c.get("text") or "").strip()
    if not body:
        return ""
    return f"### {title}\n{body}" if title else body

def _materialize_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ê° ì²­í¬ì— _text, _approx_tok í•„ë“œë¥¼ ë¶€ì—¬"""
    out: List[Dict[str, Any]] = []
    for c in chunks:
        t = _chunk_to_text(c)
        if not t:
            continue
        m = dict(c)  # ì–•ì€ ë³µì‚¬
        m["_text"] = t
        m["_approx_tok"] = max(1, len(t) // 3)  # ë¡œì»¬ ê·¼ì‚¬ì¹˜
        out.append(m)
    return out

def _shrink_text_by_chars(text: str, target_tokens: int) -> str:
    # ê·¼ì‚¬ì¹˜: 1í† í°â‰ˆ3ë¬¸ì â†’ ì—¬ìœ ë¥¼ ë‘ê³  2.8ë¡œ ë‚˜ëˆ”
    max_chars = int(target_tokens * 2.8)
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "â€¦"

def _pack_batches_by_tokens(chunks: List[Dict[str, Any]], budget_tokens: int = CONTEXT_BUDGET) -> List[List[Dict[str, Any]]]:
    """ë„¤íŠ¸ì›Œí¬ í† í¬ë‚˜ì´ì € í˜¸ì¶œ ì—†ì´, ê·¼ì‚¬ í† í°ìœ¼ë¡œë§Œ ë°°ì¹˜ êµ¬ì„±"""
    mats = _materialize_chunks(chunks)  # _text, _approx_tok í¬í•¨
    batches: List[List[Dict[str, Any]]] = []
    cur: List[Dict[str, Any]] = []
    used = 0

    for m in mats:
        cost = m["_approx_tok"] + 24  # í—¤ë” ì—¬ìœ ì¹˜
        # ê°œë³„ ì²­í¬ê°€ ê³¼ë„í•˜ê²Œ í¬ë©´ ì˜ë¼ì„œ ì‚½ì…
        if cost > int(budget_tokens * 0.9):
            short = _shrink_text_by_chars(m["_text"], int(budget_tokens * 0.9))
            m = dict(m)
            m["_text"] = short
            m["_approx_tok"] = max(1, len(short) // 3)
            cost = m["_approx_tok"] + 24

        if used + cost > budget_tokens and cur:
            batches.append(cur); cur = []; used = 0
        cur.append(m); used += cost

    if cur:
        batches.append(cur)
    return batches

def _pack_name_groups(names: List[str], budget_tokens: int = NAME_BUDGET) -> List[List[str]]:
    groups, cur, used = [], [], 0
    for n in names:
        # json.dumps ëŒ€ì‹  ê¸¸ì´ ê¸°ë°˜ ê·¼ì‚¬ (ë¬¸ì/3 + ì‰¼í‘œ ì—¬ìœ ì¹˜)
        t = max(1, len(n) // 3) + 1
        if used + t > budget_tokens and cur:
            groups.append(cur); cur, used = [], 0
        cur.append(n); used += t
    if cur:
        groups.append(cur)
    return groups

async def _wx_async(prompt: str) -> str:
    return await run_in_threadpool(_wx_generate_answer, prompt)

_CODE_FENCE_RE = re.compile(r"```(?:json)?\s*([\s\S]*?)\s*```", re.IGNORECASE)

def _sanitize_json_like(s: str) -> str:
    # í”í•œ ì˜¤ë¥˜: trailing comma ì œê±°
    s = re.sub(r",\s*(\]|\})", r"\1", s)
    # ì œì–´ë¬¸ì ì œê±°(ê°€ë” ë“¤ì–´ì˜¤ëŠ” 0-width ë“±)
    s = "".join(ch for ch in s if ch.isprintable() or ch in "\n\r\t")
    return s.strip()

def _match_json_array(s: str, start: int) -> Optional[str]:
    """startê°€ ê°€ë¦¬í‚¤ëŠ” '['ì—ì„œ ì‹œì‘í•´ ë¬¸ìì—´/ì´ìŠ¤ì¼€ì´í”„ë¥¼ ê³ ë ¤í•´
    ê· í˜•ì´ ë§ëŠ” ì²« ë°°ì—´ë§Œ ì˜ë¼ ë°˜í™˜. ì—†ìœ¼ë©´ None."""
    depth = 0
    i = start
    in_str = False
    esc = False
    while i < len(s):
        ch = s[i]
        if in_str:
            if esc:
                esc = False
            elif ch == '\\':
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == '[':
                depth += 1
            elif ch == ']':
                depth -= 1
                if depth == 0:
                    return s[start:i+1]
        i += 1
    return None

def _first_json_array_anywhere(s: str) -> Optional[List[Any]]:
    idx = s.find('[')
    while idx != -1:
        cand = _match_json_array(s, idx)
        if cand:
            txt = _sanitize_json_like(cand)
            try:
                obj = json.loads(txt)
                if isinstance(obj, list):
                    return obj
            except Exception:
                pass
        idx = s.find('[', idx + 1)
    return None

def extract_json_array(text: str) -> List[Any]:
    """ëª¨ë¸ì´ ì•ì— ì„¤ëª…ì„ ë¶™ì´ê±°ë‚˜ fenced code blockì„ ì“°ëŠ” ê²½ìš°ë„ ì•ˆì „í•˜ê²Œ ë°°ì—´ë§Œ ì¶”ì¶œ"""
    if not isinstance(text, str):
        return []

    # 1) ì½”ë“œ íœìŠ¤ ìš°ì„  (```json ... ``` ë˜ëŠ” ``` ... ```)
    for m in _CODE_FENCE_RE.finditer(text):
        block = _sanitize_json_like(m.group(1))
        # ë¸”ë¡ ìì²´ê°€ ë°°ì—´ì´ë©´ ë°”ë¡œ
        try:
            obj = json.loads(block)
            if isinstance(obj, list):
                return obj
        except Exception:
            # ë¸”ë¡ ì•ˆì— ì—¬ëŸ¬ ë‚´ìš©ì´ ì„ì˜€ìœ¼ë©´ ë°°ì—´ë§Œ ë‹¤ì‹œ ì°¾ê¸°
            arr = _first_json_array_anywhere(block)
            if arr is not None:
                return arr

    # 2) ë¬¸ì„œ ì „ì²´ì—ì„œ ì²« ì™„ê²° ë°°ì—´ë§Œ ì¶”ì¶œ
    arr = _first_json_array_anywhere(text)
    if arr is not None:
        return arr

    # 3) ë§ˆì§€ë§‰ ì‹œë„: ì•ë’¤ í† ë§‰ ì œê±° í›„ ì¬ì‹œë„
    i, j = text.find('['), text.rfind(']')
    if i >= 0 and j >= i:
        try:
            return json.loads(_sanitize_json_like(text[i:j+1])) or []
        except Exception:
            return []

    return []

# ----------------------- MAP (Watson í˜¸ì¶œ) -----------------------

async def extract_coverage_batched(base_names: List[str], chunks: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    name_groups = _pack_name_groups(base_names)
    print(name_groups)
    out: List[Dict[str, Any]] = []
    for names in name_groups:
        for b in chunks:
            prompt = _coverage_prompt(names, b)
            txt = await _wx_async(prompt)
            arr = extract_json_array(txt)
            # JSON íƒ€ì… ë°©ì–´ (ìˆ«ì ë¬¸ìì—´ -> ìˆ«ì)
            for it in arr:
                if isinstance(it, dict):
                    for k in ("coinsurance_pct","deductible_min","per_visit_limit","annual_limit","combined_cap_amount","frequency_limit","coverage_order"):
                        if k in it and isinstance(it[k], str):
                            try:
                                it[k] = float(it[k]) if "." in it[k] else int(it[k])
                            except Exception:
                                pass
            out.extend([x for x in arr if isinstance(x, dict)])
    return out

async def extract_premiums_batched(chunks: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    batches = chunks
    out: List[Dict[str, Any]] = []
    for b in batches:
        prompt = _premium_prompt(b)
        txt = await _wx_async(prompt)
        arr = extract_json_array(txt)
        for it in arr:
            if isinstance(it, dict):
                for k in ("age_min","age_max"):
                    if k in it and isinstance(it[k], str):
                        try: it[k] = int(it[k])
                        except: pass
                if "monthly_premium" in it and isinstance(it["monthly_premium"], str):
                    try: it["monthly_premium"] = float(it["monthly_premium"])
                    except: pass
        out.extend([x for x in arr if isinstance(x, dict)])
    return out

# ----------------------- REDUCE (ì„œë²„ ë³‘í•©) -----------------------

def _merge_num(a, b, mode: str):
    to_num = lambda x: None if x in (None, "", "null") else x
    a, b = to_num(a), to_num(b)
    if a is None: return b
    if b is None: return a
    if mode == "min": return a if a <= b else b
    if mode == "max": return a if a >= b else b
    return a  # default keep

def _concat_notes(*vals: Optional[str]) -> str:
    parts = [v for v in vals if isinstance(v, str) and v.strip()]
    # ì¤‘ë³µ/ê¸¸ì´ ë°©ì§€
    seen, out = set(), []
    for p in parts:
        if p not in seen:
            out.append(p); seen.add(p)
    return " | ".join(out)[:2000]

def reduce_coverage(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    (name, segment, benefit_type, combined_cap_group)ë¡œ ê·¸ë£¹í•‘í•˜ê³ 
    í•œë„ë¥˜ëŠ” max, ê³µì œ/ë³¸ì¸ë¶€ë‹´ì€ minì„ ì·¨í•¨. ì¶©ëŒì€ notesì— ê¸°ë¡.
    """
    key = lambda d: (
        d.get("name"),
        d.get("segment"),
        d.get("benefit_type"),
        d.get("combined_cap_group"),
    )
    agg: Dict[Tuple, Dict[str, Any]] = {}
    for it in items:
        k = key(it)
        if k not in agg:
            agg[k] = dict(it)
            continue
        cur = agg[k]
        # ê·œì¹™ ë³‘í•©
        cur["coinsurance_pct"] = _merge_num(cur.get("coinsurance_pct"), it.get("coinsurance_pct"), "min")
        cur["deductible_min"]  = _merge_num(cur.get("deductible_min"),  it.get("deductible_min"),  "min")
        cur["per_visit_limit"] = _merge_num(cur.get("per_visit_limit"), it.get("per_visit_limit"), "max")
        cur["annual_limit"]    = _merge_num(cur.get("annual_limit"),    it.get("annual_limit"),    "max")
        cur["combined_cap_amount"] = _merge_num(cur.get("combined_cap_amount"), it.get("combined_cap_amount"), "max")
        cur["frequency_limit"] = _merge_num(cur.get("frequency_limit"), it.get("frequency_limit"), "max")

        # coverage_orderëŠ” ê°€ì¥ ì‘ì€(ìš°ì„ ìˆœìœ„ ë†’ì€) ê°’ ìœ ì§€
        cur["coverage_order"]  = _merge_num(cur.get("coverage_order"), it.get("coverage_order"), "min")

        # source_ref, notesëŠ” í•©ì¹˜ê¸°
        cur["source_ref"] = _concat_notes(cur.get("source_ref"), it.get("source_ref"))
        cur["notes"]      = _concat_notes(cur.get("notes"), it.get("notes"))

        # currency/period ë“± ëˆ„ë½ ë³´ì™„
        for f in ("frequency_period", "combined_cap_group"):
            if not cur.get(f) and it.get(f):
                cur[f] = it[f]
    # ë¦¬ìŠ¤íŠ¸ ë°˜í™˜(ì •ë ¬ì€ coverage_order â†’ name)
    # print(lambda d: (d.get("coverage_order") or 9999, str(d.get("name") or "")))
    print(agg.values())
    return sorted(agg.values(), key=lambda d: (d.get("coverage_order") or 9999, str(d.get("name") or "")))

def reduce_premiums(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    (age_min, age_max, gender, smoker, tier)ë¡œ ê·¸ë£¹í•‘, ì›”ë³´í—˜ë£ŒëŠ” ê°’ ë‹¤ë¥´ë©´ í‰ê· /ìµœì†Ÿê°’ ì¤‘ íƒ1.
    ì—¬ê¸°ì„œëŠ” ë¬¸ì„œ ìƒ ë‹¨ì¼í‘œ ê¸°ì¤€ìœ¼ë¡œ minì„ ìœ ì§€(ë³´ìˆ˜ì ).
    """
    key = lambda d: (d.get("age_min"), d.get("age_max"), d.get("gender"), d.get("smoker"), d.get("tier") or "standard")
    agg: Dict[Tuple, Dict[str, Any]] = {}
    for it in items:
        k = key(it)
        if k not in agg:
            agg[k] = dict(it)
            continue
        cur = agg[k]
        # ê¸ˆì•¡ ì¶©ëŒ ì‹œ ë” ì‘ì€ ê°’ ìœ ì§€(ë³´ìˆ˜ì )
        if it.get("monthly_premium") is not None:
            if cur.get("monthly_premium") is None:
                cur["monthly_premium"] = it["monthly_premium"]
            else:
                cur["monthly_premium"] = min(cur["monthly_premium"], it["monthly_premium"])
        # ê·¼ê±°/ë¹„ê³  í•©ì¹˜ê¸°
        meta = cur.get("meta") or {}
        meta2 = it.get("meta") or {}
        note = _concat_notes(
            (meta.get("ë¹„ê³ ") if isinstance(meta.get("ë¹„ê³ "), str) else None),
            (meta2.get("ë¹„ê³ ") if isinstance(meta2.get("ë¹„ê³ "), str) else None),
        )
        base = _concat_notes(
            (meta.get("ê·¼ê±°") if isinstance(meta.get("ê·¼ê±°"), str) else None),
            (meta2.get("ê·¼ê±°") if isinstance(meta2.get("ê·¼ê±°"), str) else None),
        )
        cur["meta"] = {"ê·¼ê±°": base, "ë¹„ê³ ": note}
        # currency ê¸°ë³¸ê°’
        if not cur.get("currency"):
            cur["currency"] = it.get("currency") or "KRW"

    return sorted(agg.values(), key=lambda d: (d.get("tier") or "standard", d.get("gender") or "A", d.get("age_min") or 0))

def _chunks_to_context(chunks: List[Dict[str, Any]], max_chars: int = 100_000) -> str:
    """section_title, content/embed_input/text ìˆœìœ¼ë¡œ ë¬¸ë§¥ í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±° í¬í•¨)."""
    parts: List[str] = []
    for c in chunks:
        title = (c.get("section_title") or "").strip()
        body = (c.get("content") or c.get("embed_input") or c.get("text") or "").strip()
        if not body:
            continue
        if title:
            parts.append(f"### {title}\n{body}")
        else:
            parts.append(body)
    # ì¤‘ë³µ ì œê±°
    seen, uniq = set(), []
    for p in parts:
        if p not in seen:
            uniq.append(p); seen.add(p)
    ctx = "\n\n".join(uniq)
    return ctx[:max_chars]

def _coverage_prompt(base_names: List[str], chunks: List[Dict[str, Any]]) -> str:
    names_json = json.dumps(base_names, ensure_ascii=False)
    context = _chunks_to_context(chunks)
    return (
        f"{COVERAGE_SYS}\n\n"
        f"[coverage_item.name ëª©ë¡]\n{names_json}\n\n"
        f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
        "[ì§€ì‹œì‚¬í•­]\n"
        "- JSON ë°°ì—´ë§Œ ì¶œë ¥. ì£¼ì„/ì„¤ëª…/ë¼ë²¨ ê¸ˆì§€.\n"
        "- ìˆ˜ì¹˜ëŠ” KRW ë‹¨ìœ„. ë²”ìœ„/â€˜~â€™ í‘œê¸°ëŠ” ë‹¨ì¼ ìˆ˜ì¹˜ë¡œ ì •ê·œí™”.\n"
        "- ê° í•­ëª©ì— source_refë¡œ ê·¼ê±° êµ¬ì ˆ/í˜ì´ì§€ë¥¼ ë‚¨ê²¨."
    )

def _premium_prompt(chunks: List[Dict[str, Any]]) -> str:
    context = _chunks_to_context(chunks)
    return (
        f"{PREMIUM_SYS}\n\n"
        f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
        "[ì§€ì‹œì‚¬í•­]\n"
        "- JSON ë°°ì—´ë§Œ ì¶œë ¥. ì£¼ì„/ì„¤ëª…/ë¼ë²¨ ê¸ˆì§€.\n"
        "- ë¬¸ì„œì— ì‹¤ì œ ê¸ˆì•¡ ìˆì„ ë•Œë§Œ monthly_premium ì±„ì›€."
    )

async def extract_policy_meta(chunks: List[List[Dict[str, Any]]]) -> Dict[str, Any]:
    flat = [c for b in chunks for c in b]
    prompt = _policy_meta_prompt(flat)
    txt = await _wx_async(prompt)
    arr = extract_json_array(txt)
    for it in arr:
        if isinstance(it, dict):
            for k in ("waiting_period_days", "age_min", "age_max"):
                if k in it and isinstance(it[k], str):
                    try: it[k] = int(it[k])
                    except: it[k] = None
            if "is_catalog" in it and isinstance(it["is_catalog"], str):
                it["is_catalog"] = it["is_catalog"].lower() in {"true", "1", "yes"}
            return it
    return {}

def _policy_meta_prompt(chunks: List[Dict[str, Any]]) -> str:
    context = _chunks_to_context(chunks)
    return (
        f"{POLICY_META_SYS}\n\n"
        f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
        "[ì§€ì‹œì‚¬í•­]\n"
        "- JSON ë°°ì—´ë§Œ ì¶œë ¥. ì£¼ì„/ì„¤ëª…/ë¼ë²¨ ê¸ˆì§€."
    )
