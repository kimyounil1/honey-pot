# app/rag/retriever.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple

from app.services import vector_db  # OpenSearch client (assumed existing)
import os
import logging
import boto3

from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth
from app.services.common import Mode
from app.config import settings

# watsonx.ai (pip install ibm-watsonx-ai)
try:
    from ibm_watsonx_ai import Credentials, APIClient
    from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
    from ibm_watsonx_ai.foundation_models import ModelInference
except Exception:
    Credentials = None
    APIClient = None
    ModelInference = None

logger = logging.getLogger(__name__)

# ============================= OpenSearch =============================

def _os_client() -> OpenSearch:
    region = getattr(settings, "OPENSEARCH_REGION", os.getenv("OPENSEARCH_REGION", "us-east-1"))
    auth = AWSV4SignerAuth(boto3.Session().get_credentials(), region, "es")
    host = getattr(settings, "OPENSEARCH_HOST", os.getenv("OPENSEARCH_HOST"))
    port = int(getattr(settings, "OPENSEARCH_PORT", os.getenv("OPENSEARCH_PORT", "443")))
    return OpenSearch(
        hosts=[{"host": host, "port": port}],
        http_auth=auth,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection,
    )

def _trim(s: str, n: int = 120) -> str:
    s = (s or "").replace("\n", " ").strip()
    return (s[:n] + "â€¦") if len(s) > n else s

def _search_snippets(query: str, k: int = 8) -> List[Dict[str, Any]]:
    index = getattr(settings, "OPENSEARCH_INDEX", None)

    body = {
        "size": max(1, min(k, 20)),
        "query": {
            "multi_match": {
                "query": query,
                "fields": ["section_title^2", "content"],
                "type": "best_fields",
                "tie_breaker": 0.2
            }
        },
        "_source": True,
    }
    resp = _os_client().search(index=index, body=body)

    # total íŒŒì‹±(ë²„ì „ì— ë”°ë¼ dict/int í˜¼ì¬)
    raw_total = (resp.get("hits", {}).get("total", {}) or {})
    if isinstance(raw_total, dict):
        total = int(raw_total.get("value", 0))
    else:
        total = int(raw_total or 0)

    hits = resp.get("hits", {}).get("hits", []) or []

    if not hits:
        # âœ… ê²€ìƒ‰ 0ê±´ ë¡œê·¸
        logger.info(
            "[RAG][OS_EMPTY] index=%s total=%d k=%d query=%s",
            index, total, k, _trim(query)
        )
        return []

    out = []
    for h in hits:
        s = h.get("_source", {}) or {}
        out.append({
            "score": h.get("_score", 0.0),
            "section_title": s.get("section_title", "") or "",
            "content": s.get("content", "") or "",
            "filename": s.get("filename", ""),
            "insurer": s.get("insurer", ""),
            "version": s.get("version", ""),
            "policy_id": s.get("policy_id", "") or s.get("policy", ""),
            "effective_date": s.get("effective_date", ""),
        })

    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë¡œê·¸(ìƒìœ„ 1ê°œë§Œ ì œëª© ì°ê¸°)
    logger.debug(
        "[RAG][OS_OK] index=%s total=%d k=%d top_title=%s query=%s",
        index, total, k, _trim(out[0].get("section_title")), _trim(query)
    )
    return out

# ============================= Token Utils =============================

def _rough_tokens(text: str) -> int:
    """ë³´ìˆ˜ ê·¼ì‚¬(í•œê¸€ í¬í•¨): ~1í† í°=3ë¬¸ì."""
    if not text:
        return 0
    return max(1, len(text) // 3)

def _wx_model() -> Optional[ModelInference]:
    if ModelInference is None:
        logger.warning("ibm-watsonx-ai SDK not installed.")
        return None

    creds = Credentials(api_key=settings.WATSONX_API_KEY, url=settings.WATSONX_URL)
    _ = APIClient(creds)  # ë‚´ë¶€ í† í¬ë‚˜ì´ì € í˜¸ì¶œì— í•„ìš”

    return ModelInference(
        model_id=settings.WATSONX_MODEL_ID,
        credentials=creds,
        space_id=settings.WATSONX_SPACE_ID,
    )

def _wx_token_count(model: Optional[ModelInference], text: str) -> int:
    if not model:
        return _rough_tokens(text)
    try:
        t = model.tokenize(prompt=text, return_tokens=True)
        return len(t.get("result", []))
    except Exception:
        return _rough_tokens(text)

def _fit_snippets_to_limit(
    snippets: List[Dict[str, Any]],
    user_query: str,
    token_limit: int = 30000,
    system_overhead_tokens: int = 600,
    per_snippet_header_tokens: int = 12,
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    watsonx í˜¸ì¶œ ì „, RAG ì»¨í…ìŠ¤íŠ¸(ì›ë¬¸ ìŠ¤ë‹ˆí«) ë¸”ë¡ì„ 10,000 í† í° ì´ë‚´ë¡œ êµ¬ì„±.
    watsonx í† í¬ë‚˜ì´ì € ì—†ì´ ë³´ìˆ˜ ê·¼ì‚¬ ì‚¬ìš©.
    """
    header = "[RAG CONTEXT]\n"
    used = _rough_tokens(header) + _rough_tokens(user_query) + system_overhead_tokens
    chosen: List[Dict[str, Any]] = []
    lines: List[str] = [header]

    for s in snippets:
        title = s.get("section_title") or "snippet"
        body = (s.get("content") or "").strip()
        if not body:
            continue
        block = f"- ({title}) {body}"
        need = per_snippet_header_tokens + _rough_tokens(block)

        if used + need > token_limit:
            remain = token_limit - used - per_snippet_header_tokens
            if remain <= 0:
                break
            approx_chars = max(0, remain * 3)
            block = f"- ({title}) {body[:approx_chars]}â€¦"
            need = per_snippet_header_tokens + _rough_tokens(block)
            if used + need > token_limit:
                break

        lines.append(block)
        used += need
        chosen.append(s)

    # ê°„ë‹¨ ì¶œì²˜
    if chosen:
        labels = []
        for s in chosen:
            parts = []
            if s.get("insurer"): parts.append(str(s["insurer"]))
            if s.get("version"): parts.append(str(s["version"]))
            if s.get("filename"): parts.append(str(s["filename"]))
            if s.get("policy_id"): parts.append(f"#{s['policy_id']}")
            lab = " ".join([p for p in parts if p])
            if lab and lab not in labels:
                labels.append(lab)
        if labels:
            lines.append("ğŸ” ì¶œì²˜: " + " / ".join(labels))

    return "\n".join(lines), chosen

# ============================= watsonx Generator =============================

def _rag_prompt(user_query: str, context_block: str) -> str:
    return (
        "ë‹¹ì‹ ì€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ë‹µí•˜ëŠ” ë³´í—˜ ë„ë©”ì¸ RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ê·œì¹™:\n"
        "1) ì»¨í…ìŠ¤íŠ¸ ë°– ì§€ì‹ ì‚¬ìš© ê¸ˆì§€.\n"
        "2) ê¸ˆì•¡/í•œë„/ë©´ì±… ë“± ìˆ˜ì¹˜ëŠ” ì¸ìš© ê·¼ê±°ì™€ í•¨ê»˜ ì œì‹œ.\n"
        "3) ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œ.\n\n"
        f"{context_block}\n\n"
        f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_query}\n\n"
        "ì¶œë ¥ í˜•ì‹: ìš”ì•½ â†’ ê·¼ê±°(ê¸€ë¨¸ë¦¬í‘œ) â†’ ìœ ì˜ì‚¬í•­ â†’ ì¶œì²˜"
    )

def _wx_generate_answer(prompt: str) -> str:
    model = _wx_model()
    if model is None:
        return ""
    try:
        gen_params = {
            GenParams.MAX_NEW_TOKENS: 4000,
            GenParams.TEMPERATURE: 0.2,
            GenParams.DECODING_METHOD: "greedy",
            GenParams.REPETITION_PENALTY: 1.05,
            GenParams.STOP_SEQUENCES: [],
        }
        resp = model.generate_text(prompt=prompt, params=gen_params)

        return (resp or "").strip()
    except Exception as e:
        logger.exception("watsonx.ai generation failed: %s", e)
        return ""

# ============================= Public API =============================
def retrieve(
    *,
    mode,
    user_id: str,
    query: str,
    attachment_ids: List[str] | None = None,
    product_id: Optional[str] = None,
    limit: int = 5,
    fallback_to_global: bool = False,
) -> str:
    # 1) product_idê°€ ìˆìœ¼ë©´: ê·¸ ìƒí’ˆìœ¼ë¡œë§Œ ê²€ìƒ‰
    if product_id:
        docs = vector_db.search_documents(query, product_id=product_id, limit=limit) or []
        snippets = [(d.get("content") or d.get("text") or d.get("embed_input") or "").strip() for d in docs]
        snippets = [s for s in snippets if s]
        if snippets:
            return "\n\n".join(snippets)
        # ìƒí’ˆ ìŠ¤ì½”í”„ì—ì„œ ëª» ì°¾ì•˜ê³ , ì „ì—­ í´ë°±ì„ í—ˆìš©í•œ ê²½ìš°
        if fallback_to_global:
            return _retrieve_global(mode=mode, user_id=user_id, query=query, attachment_ids=attachment_ids)
        # í´ë°± ë¯¸í—ˆìš©ì´ë©´ ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜(ìƒìœ„ì—ì„œ ì²˜ë¦¬)
        return ""

    # 2) product_idê°€ ì—†ìœ¼ë©´: ë¬´ì¡°ê±´ ì „ì—­ ê²€ìƒ‰ìœ¼ë¡œ
    return _retrieve_global(mode=mode, user_id=user_id, query=query, attachment_ids=attachment_ids)

def _retrieve_global(*, mode, user_id: str, query: str, attachment_ids: List[str] | None = None) -> str:
    """
    Auto-RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„±ê¸°:
    - OpenSearchì—ì„œ ìŠ¤ë‹ˆí« ê²€ìƒ‰ â†’ 10,000 í† í° í•œë„ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    - watsonxê°€ í•´ë‹¹ ì»¨í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì´ˆì•ˆ ë‹µë³€ ìƒì„±
    - ìƒì„±ëœ ì´ˆì•ˆ ë‹µë³€ì„ '[RAG AUTO ANSWER]' ë¸”ë¡ìœ¼ë¡œ ë°˜í™˜
    - watsonx ë¯¸ì„¤ì¹˜/ì˜¤ë¥˜ ì‹œ, ì»¨í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ ë°˜í™˜(íŒŒì´í”„ë¼ì¸ ìœ ì§€)
    """
    try:
        snippets = _search_snippets(query=query)
        context_block, _ = _fit_snippets_to_limit(
            snippets=snippets,
            user_query=query,
        )
        prompt = _rag_prompt(user_query=query, context_block=context_block)
        print("[RAG AUTO PROMPT END]\n" + prompt)
        answer = _wx_generate_answer(prompt)
        print("[RAG AUTO ANSWER END]\n" + answer)
        if (answer or "").strip():
            # ì™¸ë¶€(OpenAI) í´ë¦¬ì‹±ì´ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëª…í™•í•œ í—¤ë” ë¶€ì—¬
            return "[RAG AUTO ANSWER]\n" + answer

        # watsonx ì‹¤íŒ¨ ì‹œ: ì›ë¬¸ ì»¨í…ìŠ¤íŠ¸ë¼ë„ ë°˜í™˜
        return context_block

    except Exception as e:
        logger.exception("retrieve failed: %s", e)
        return ""

#
from typing import Dict, Any

async def policy_db_lookup(*, mode: Mode, entities: Dict[str, Any], user_text: str) -> str:
    """
    TODO: ì‹¤ì œ ì•½ê´€ DB ì§ì¡°íšŒ ë¡œì§ìœ¼ë¡œ êµì²´.
    í˜„ì¬ëŠ” ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ì—¬ stageì—ì„œ RAG ë³´ì¡°ë¥¼ ì‹œë„í•˜ê²Œ ë‘¡ë‹ˆë‹¤.
    """
    try:
        return ""
    except Exception as e:
        logger.warning("policy_db_lookup failed: %s", e)
        return ""